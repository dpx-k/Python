{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb74ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e80c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "# load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# split the dataset into features and labels \n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# print(X, y)\n",
    "\n",
    "print(X.shape) # 150 * 4 array \n",
    "print(y.shape) # 150 elements linear array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4734ed5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(30, 4)\n",
      "(120,)\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "# split the data into train and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0315497f",
   "metadata": {},
   "source": [
    "# Scikit-learn's Inbuilt Datasets\n",
    "\n",
    "Yes, **scikit-learn (sklearn) comes with many inbuilt datasets!** This is a fantastic feature for learning, experimenting, and quickly prototyping machine learning models. These datasets are readily available through the `sklearn.datasets` module.\n",
    "\n",
    "They generally fall into a few helpful categories:\n",
    "\n",
    "* **Toy Datasets:** These are small, simple datasets perfect for educational purposes, demonstrating how algorithms work, and quick tests.\n",
    "    * Examples: `load_iris`, `load_diabetes`, `load_digits`, `load_wine`, `load_breast_cancer`, `load_linnerud`.\n",
    "* **Real-world Datasets:** These are larger datasets that get downloaded the first time you access them.\n",
    "    * Examples: `fetch_20newsgroups`, `fetch_california_housing`, `fetch_olivetti_faces`.\n",
    "    * **Note:** These require an internet connection the very first time you use them, as they aren't bundled directly with the scikit-learn installation.\n",
    "* **Generated Datasets:** These are functions that create synthetic datasets with specific properties. They're super useful for testing algorithms under controlled conditions, especially when you want to see how a model performs with different data characteristics.\n",
    "    * Examples: `make_classification`, `make_regression`, `make_blobs`.\n",
    "\n",
    "---\n",
    "\n",
    "## What is the Iris Dataset?\n",
    "\n",
    "The **Iris dataset** is arguably the most famous and widely used \"toy\" dataset in machine learning, particularly for classification problems. It's often referred to as the \"Hello World\" of machine learning because it's simple enough to understand quickly but complex enough to showcase various classification techniques.\n",
    "\n",
    "Here's what makes it so significant:\n",
    "\n",
    "* **Origin:** It was introduced by the renowned British statistician and biologist Ronald Fisher in 1936.\n",
    "* **Purpose:** It serves as a classic benchmark for demonstrating and testing classification algorithms due to its straightforward nature and well-defined classes.\n",
    "* **Structure:**\n",
    "    * **Instances (samples):** It contains 150 total data points.\n",
    "    * **Features (attributes):** Each instance has 4 numerical features, all measured in centimeters:\n",
    "        1.  Sepal length\n",
    "        2.  Sepal width\n",
    "        3.  Petal length\n",
    "        4.  Petal width\n",
    "    * **Classes (targets):** The data represents 3 distinct species of Iris flowers, with 50 instances for each species:\n",
    "        1.  Iris Setosa\n",
    "        2.  Iris Versicolor\n",
    "        3.  Iris Virginica\n",
    "* **Characteristics:**\n",
    "    * One class (`Iris Setosa`) is **linearly separable** from the other two, meaning you can draw a straight line (or plane) to separate it from the rest.\n",
    "    * The other two classes (`Iris Versicolor` and `Iris Virginica`) are **not perfectly linearly separable** from each other. This slight overlap adds a touch of challenge and makes the dataset interesting for evaluating more sophisticated classification algorithms beyond basic linear models.\n",
    "\n",
    "---\n",
    "\n",
    "## How to Load the Iris Dataset in Scikit-learn\n",
    "\n",
    "Loading the Iris dataset is straightforward. Here's how you do it and how to inspect its structure:\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# The 'data' attribute holds the features (X)\n",
    "X = iris.data\n",
    "\n",
    "# The 'target' attribute holds the labels (y)\n",
    "y = iris.target\n",
    "\n",
    "# You can also access descriptive information about the dataset\n",
    "feature_names = iris.feature_names # Names of the features\n",
    "target_names = iris.target_names   # Names of the target classes\n",
    "\n",
    "print(\"Features (X) shape:\", X.shape)\n",
    "print(\"Target (y) shape:\", y.shape)\n",
    "print(\"Feature names:\", feature_names)\n",
    "print(\"Target names:\", target_names)\n",
    "\n",
    "print(\"\\nFirst 5 samples of features (X):\\n\", X[:5])\n",
    "print(\"First 5 samples of target labels (y):\\n\", y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bae53b",
   "metadata": {},
   "source": [
    "# Understanding `train_test_split` in Scikit-learn\n",
    "\n",
    "The `train_test_split` function is a fundamental utility in machine learning workflows, used to divide your dataset into training and testing subsets.\n",
    "\n",
    "---\n",
    "\n",
    "## Why We Split Data\n",
    "\n",
    "The primary goal of splitting your data is to accurately evaluate your machine learning model's performance.\n",
    "\n",
    "* **Training Set:** This portion of the data is used to **train your machine learning model**. The model learns patterns and relationships from these examples.\n",
    "* **Testing Set:** This independent portion of the data is used to **evaluate the performance of your trained model on unseen data**. This is crucial for assessing how well your model **generalizes** to new, real-world examples and helps in **preventing overfitting**.\n",
    "\n",
    "The function takes your input **features `X`** (independent variables) and your **target variable `y`** (dependent variable/labels) and splits them consistently, ensuring that corresponding rows in `X` and `y` stay together.\n",
    "\n",
    "---\n",
    "\n",
    "## Understanding the Parameters\n",
    "\n",
    "Let's break down the key parameters of the `train_test_split` function:\n",
    "\n",
    "* **`X`**: Your **feature matrix** (e.g., `iris.data`). This contains the input features for your model.\n",
    "* **`y`**: Your **target vector** (e.g., `iris.target`). This contains the labels or values your model is trying to predict.\n",
    "* **`test_size`**:\n",
    "    * If a `float` (e.g., `0.2` or `0.3`), it represents the **proportion** of the dataset to allocate to the test split. So, `0.2` means 20% of the data goes into the test set.\n",
    "    * If an `int`, it represents the **absolute number** of samples to include in the test split.\n",
    "    * **Default:** If neither `test_size` nor `train_size` is specified, `test_size` defaults to `0.25` (25%).\n",
    "* **`train_size`**:\n",
    "    * If a `float`, it represents the **proportion** of the dataset to allocate to the training split.\n",
    "    * If an `int`, it represents the **absolute number** of samples to include in the training split.\n",
    "    * **Default:** If neither `test_size` nor `train_size` is specified, its value is inferred from `test_size`.\n",
    "* **`random_state`**:\n",
    "    * An `integer` or `None`. This parameter is vital for **reproducibility**. Setting it to a specific integer (e.g., `42`) ensures that your data split will be the same every time you run your code. If left as `None` (the default), the split will differ with each execution due to a changing random number generator.\n",
    "* **`shuffle`**:\n",
    "    * `bool`, default=`True`. Determines whether the data is **shuffled** before being split. Shuffling is almost always recommended to ensure both the training and test sets are representative of the overall dataset and don't carry any inherent ordering biases.\n",
    "* **`stratify`**:\n",
    "    * `array-like` or `None`, default=`None`. If provided, this parameter should be your `y` array. It ensures that the **proportions of class labels** in both the training and testing sets are roughly the same as those in the original full dataset. This is especially useful for **imbalanced datasets** to prevent a situation where one class is over-represented in one split and under-represented in the other.\n",
    "\n",
    "---\n",
    "\n",
    "## Why You Typically Don't Use Both `test_size` and `train_size` Simultaneously\n",
    "\n",
    "The reason you usually specify only one of `test_size` or `train_size` is that **they are complementary**. If you define one, the other is automatically determined.\n",
    "\n",
    "For example, with a dataset of 100 samples:\n",
    "* If you set `test_size = 0.2`, 20 samples go to the test set, and by implication, 80 samples (`100 - 20`) form the training set. The `train_size` is implicitly `0.8`.\n",
    "* Similarly, if you set `train_size = 0.8`, 80 samples go to the training set, and 20 samples (`100 - 80`) form the test set. The `test_size` is implicitly `0.2`.\n",
    "\n",
    "Specifying both values when they might conflict can lead to errors. For instance, if you provide `test_size=0.3` and `train_size=0.6`, their sum is `0.9`, leaving 10% of the data unaccounted for. Scikit-learn will typically raise a `ValueError` in such cases unless their sum exactly equals 1.0 (for float proportions) or the total number of samples (for integer counts).\n",
    "\n",
    "The function is designed for convenience: just specify the split size you care about most, and it handles the rest. Most users think in terms of how much data they want to hold out for testing, which is why `test_size` is more commonly used.\n",
    "\n",
    "---\n",
    "\n",
    "## Example Usage\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset (a common toy dataset)\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "print(f\"Original dataset shape: {X.shape}\")\n",
    "\n",
    "# 1. Common usage: Specifying test_size (20% for testing)\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"\\n--- Using test_size = 0.2 ---\")\n",
    "print(f\"X_train_1 shape: {X_train_1.shape}\") # (120, 4)\n",
    "print(f\"X_test_1 shape: {X_test_1.shape}\")   # (30, 4)\n",
    "\n",
    "# 2. Alternatively: Specifying train_size (80% for training)\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "print(\"\\n--- Using train_size = 0.8 ---\")\n",
    "print(f\"X_train_2 shape: {X_train_2.shape}\") # (120, 4)\n",
    "print(f\"X_test_2 shape: {X_test_2.shape}\")   # (30, 4)\n",
    "# Notice that the shapes are identical to the first example because the splits are complementary.\n",
    "\n",
    "# 3. Using both (when they are consistent – sums to 1.0)\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=42)\n",
    "print(\"\\n--- Using both (consistent) ---\")\n",
    "print(f\"X_train_3 shape: {X_train_3.shape}\") # (120, 4)\n",
    "print(f\"X_test_3 shape: {X_test_3.shape}\")   # (30, 4)\n",
    "\n",
    "# 4. Attempting to use both with inconsistent values (will raise a ValueError)\n",
    "try:\n",
    "    X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(X, y, test_size=0.3, train_size=0.6, random_state=42)\n",
    "except ValueError as e:\n",
    "    print(f\"\\n--- Error with inconsistent test_size and train_size ---\")\n",
    "    print(e)\n",
    "    # Expected Output: ValueError: The sum of test_size and train_size = 0.9, but should be at most 1.0. Reduce test_size or train_size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6230ae5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".cortex_engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
